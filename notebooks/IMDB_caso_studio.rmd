---
title: "IMDB caso di studio"
author: "Alessandro Aere"
date: "22 maggio 2020"
output: rmarkdown::html_vignette
---

# Caso di studio: *sentiment analysis*

## Descrizione del problema

Verrà effettuato un confronto di modelli utilizzando il dataset IMDB, una raccolta di recensioni di film.

  * Lo scopo è determinare se una recensione è positiva o negativa (classificazione binomiale).
  * L'insieme di stima è composto da $25 000$ osservazioni (recensioni).
  * Ci sono $10 000$ variabili esplicative e rappresentano le parole presenti nell'articolo; ogni variabile indica la presenza o assenza di una determinata parola. (eccetto recurrent neural network)
  * Verrà calcolata l'accuratezza nell'insieme di verifica, composto da $25 000$ osservazioni.
  
Per maggiori dettagli riguardo i dati, consultare questo [link](https://ai.stanford.edu/~amaas/data/sentiment/).

In questo report verrà riportato solamente il codice riguardante le reti neurali, e non quello di tutti gli altri modelli statistici, in quanto non sono argomento di interesse. Nelle conclusioni potrete trovare, tuttavia, il confronto dei risultati.

## Caricamento della libreria Keras

```{r, results="hide", message=FALSE}
devtools::install_github("rstudio/keras")
library(keras)
install_keras()
```

Eseguite il codice qui in seguito, se desiderate impostare in *seed* ed ottenere risultati riproducibili.

```{r}
library(reticulate)
py_run_string("import numpy as np;
import tensorflow as tf;
import random as python_random;
np.random.seed(123);
python_random.seed(123);
tf.random.set_seed(123);")
```

## Caricamento dei dati

In seguito, definiamo due variabili necessarie per la *sentiment analysis*, o in generale l'analisi testuale:

  * `max_features`: generalmente quando si effettua un'analisi testuale, si fa riferimento ad un vocabolario, ovvero una raccolta di parole ordinata secondo la frequenza in cui si presentano nel corpus di documenti; di queste parole vengono considerate solamente le *n* parole più frequenti, dove *n* è il valore associato alla variabile `max_features`; tutte le restanti parole vengono sostituite con un generico valore `2L`.
  * `maxlen`: è la lunghezza massima di una frase; se una frase è più lunga del valore di questa variabile, verrà tagliata.

```{r}
max_features <- 5000
maxlen <- 500

imdb <- dataset_imdb(num_words = max_features)
c(c(x_train, train_labels), c(x_test, test_labels)) %<-% imdb
```

## Preparazione dei dati

Generalmente il primo step di preparazione è la **tokenizzazione**, ossia l'associazione di un numero intero a ciascuna parola. Questo step non è necessario, perchè in questo caso le sequenze sono già *tokenizzate*.

In seguito, defininiamo anche una funzione che servirà in seguito: `vectorize_sequences`. Questa funzione ha come scopo la trasformazione dell'intero dataset, effettuando la dicotomizzazione delle parole presenti nelle frasi. 

```{r}
vectorize_sequences <- function(sequences, dimension = max_features) {
# Function that applies one-hot-encoding to the sequences.
# Input:
# - sequences: list of sequences; each sequence has length equal to the length of the sentence, and each int value of the sequence corresponds to a word.
# - dimension: max number of words to include; words are ranked by how often they occur (in the training set) and only the most frequent words are kept.
# Output:
# - results: matrix of dim = (n, dimension); each row represents a different sentence, and each column represents a different word; the matrix is filled by 0 and 1 values, depending if the word is present in that sentence or else.
results <- matrix(0, nrow = length(sequences), ncol = dimension)
for (i in 1:length(sequences)) results[i, sequences[[i]]] <- 1
return(results)
}

y_train <- as.numeric(train_labels)
y_test <- as.numeric(test_labels)
```

## Deep Neural Network

Specificare l'architettura della rete neurale:

  * vengono utilizzati due layer *fully-connected*, il primo con $32$ nodi ed il secondo con $16$.
  * viene utilizzata la funzione *ReLU* come funzione di attivazione negli strati latenti, e la funzione *logistica* nello strato di output. 

```{r, message=FALSE, warning=FALSE}
model <- keras_model_sequential() %>%
layer_dense(units = 32, activation = "relu", input_shape = c(max_features)) %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")

model
```

Compilare il modello:

  * l'ottimizzatore utilizzato è `rmsprop`, con un *learning rate* pari a $0.001$;
  * viene utilizzata la *cross-entropia* come funzione di perdita, e l'*accuratezza* come metrica di valutazione.

```{r}
model %>% compile(
  optimizer = optimizer_rmsprop(lr = 0.001),
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```

Effettuare il training della rete neurale. Come input alla rete neurale, diamo le sequenze dicotomizzate, applicando la funzione `vectorize_sequences` al dataset `x_train`. 

```{r echo=TRUE, warning=FALSE}
history <- model %>% fit(
  x = vectorize_sequences(x_train), 
  y = y_train,
  epochs = 10,
  batch_size = 32,
  validation_split = 0.2,
  verbose = 1
)
```

In seguito viene rappresentato il grafico della funzione di perdita e dell'accuratezza in funzione del numero di epoche.

```{r, fig.height=5, fig.width=7, out.width='100%', message=FALSE}
plot(history)
```

Valutazione del modello sui dati di test.

```{r, message = FALSE}
results <- model %>% evaluate(
  x = vectorize_sequences(x_test), 
  y = y_test,
  verbose = 0
)

print(paste("Loss on test data:", results["loss"]))
print(paste("Accuracy on test data:", results["accuracy"]))
```

## Recurrent neural network (lstm)

Specificare l'architettura della rete neurale:

  * Il primo strato è un layer di *embedding*, con il quale ogni parola viene trasformata in un vettore di dimensione pari a $128$
  * Il secondo strato è un layer **LSTM**, una versione potenziata del layer ricorrente.

```{r, message=FALSE, warning=FALSE}
model <- keras_model_sequential() %>%
layer_embedding(input_dim = max_features, output_dim = 128) %>%
layer_lstm(units = 32) %>%
layer_dense(units = 1, activation = "sigmoid")

model
```

Compilare il modello:

  * l'ottimizzatore utilizzato è `rmsprop`, con un *learning rate* pari a $0.001$;
  * viene utilizzata la *cross-entropia* come funzione di perdita, e l'*accuratezza* come metrica di valutazione.

```{r}
model %>% compile(
  optimizer = optimizer_rmsprop(lr = 0.001),
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```

Effettuare il training della rete neurale. Prima di dare in input le sequenze, vi applichiamo la funzione `pad_sequences`: questa funzione applica una lunghezza massima alle sequenze, in questo caso pari al valore *max_len*. Inoltre, alle sequenze più corte di *max_len*, vi applica il **padding**, ossia aggiunge una serie di elementi tutti pari a $0$ (zero) al temine della sequenza, fino a farla diventare di lunghezza *max_len*. Questa tecnica è necessaria perchè la rete neurale richiede che le sequenze in input abbiano tutte la stessa lunghezza.

```{r echo=TRUE}
history <- model %>% fit(
  x = pad_sequences(x_train, maxlen = maxlen), 
  y = y_train,
  epochs = 10,
  batch_size = 32,
  validation_split = 0.2,
  verbose = 1
)
```

In seguito viene rappresentato il grafico della funzione di perdita e dell'accuratezza in funzione del numero di epoche.

```{r, fig.height=5, fig.width=7, out.width='100%', message=FALSE}
plot(history)
```

Valutazione del modello sui dati di test.

```{r}
results <- model %>% evaluate(
  x = pad_sequences(x_test, maxlen = maxlen), 
  y = y_test,
  verbose = 0
)

print(paste("Loss on test data:", results["loss"]))
print(paste("Accuracy on test data:", results["accuracy"]))
```

## Risultati

| **Modello**              | **Accuratezza** |
|--------------------------|-----------------|
| Recurrent neural network | 88.4%           |
| Lasso                    | 87.0%           |
| Regressione logistica    | 86.2%           |
| Deep neural network      | 86.1%           |
| Random forest            | 84.7%           |
| Bagging                  | 77.0%           |
| Adaboost                 | 72.5%           |
| Gradient boosting        | 70.1%           |
